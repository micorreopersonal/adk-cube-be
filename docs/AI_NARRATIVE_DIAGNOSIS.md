# DiagnÃ³stico: Narrativas AI No Disponibles

## ğŸ” Causa RaÃ­z Identificada

**Error 429 - RESOURCE_EXHAUSTED (Cuota de API Excedida)**

Las narrativas del reporte ejecutivo no se generan porque las llamadas a Gemini (Vertex AI) estÃ¡n siendo rechazadas por exceder la cuota de requests por minuto (RPM) o tokens por minuto (TPM).

---

## ğŸ“Š Evidencia del DiagnÃ³stico

### Test Ejecutado:
```bash
python tests/test_direct_insight.py
```

### Logs Capturados:
```
2026-02-09 13:41:30 - app.ai.tools.executive_insights - ERROR - Error generating insight
'status': 'RESOURCE_EXHAUSTED'
error-code-429
```

### Resultado:
- **4/4 narrativas** retornaron `"[AI Narrative Unavailable]"`
- Causa: El decorador `@retry` intenta 3 veces, pero todas fallan con 429

---

## ğŸ§¬ AnÃ¡lisis del CÃ³digo Actual

### `executive_insights.py` - LÃ­neas 80-98

```python
@retry(
    retry=retry_if_exception_type(google.api_core.exceptions.ResourceExhausted),
    stop=stop_after_attempt(3),           # Solo 3 intentos
    wait=wait_exponential(multiplier=1, min=2, max=10),  # Espera: 2s, 4s, 8s (muy corto)
    reraise=True
)
def _generate_with_retry(self, prompt: str, max_tokens: int) -> str:
    response = self.client.models.generate_content(...)
    return response.text.strip()
```

**Problema:**
- La espera mÃ¡xima entre reintentos es **10 segundos**
- Con cuota agotada, el sistema puede necesitar **30-60 segundos** para resetear
- DespuÃ©s de 3 fallos, lanza excepciÃ³n que es capturada en lÃ­nea 119 y retorna `"[AI Narrative Unavailable - Quota Exceeded]"`

---

## ğŸ¯ Soluciones Propuestas

### SoluciÃ³n 1: **Aumentar Retry Wait Time** (Corto Plazo) âš¡

Modificar el decorador de retry para esperar mÃ¡s tiempo:

```python
@retry(
    retry=retry_if_exception_type(google.api_core.exceptions.ResourceExhausted),
    stop=stop_after_attempt(5),           # 5 intentos en lugar de 3
    wait=wait_exponential(multiplier=2, min=5, max=60),  # Espera: 5s, 10s, 20s, 40s, 60s
    reraise=True
)
```

**Beneficio:** Aumenta la probabilidad de Ã©xito cuando hay picos temporales de trÃ¡fico

---

### SoluciÃ³n 2: **Rate Limiting entre Secciones** (Medio Plazo) ğŸ›ï¸

Agregar delays intencionales entre la generaciÃ³n de narrativas:

```python
# En executive_report_orchestrator.py
import time

# DespuÃ©s de cada generaciÃ³n de insight:
insight_1 = ai_gen.generate_section_insight("critical_insight", ...)
time.sleep(2)  # Esperar 2 segundos antes de la siguiente llamada
```

**Beneficio:** Distribuye las llamadas a la API en el tiempo, evitando burst

---

### SoluciÃ³n 3: **Caching Agresivo** (ArquitectÃ³nico) ğŸ’¾

El sistema ya tiene caching en Firestore, pero podemos mejorarlo:

#### 3.1 Aumentar TTL de Cache
```python
# LÃ­nea 60 en executive_insights.py - Actualmente comentado
# Activar TTL de 7 dÃ­as para evitar regenerar durante la semana
if created_at and (datetime.now(created_at.tzinfo) - created_at).days > 7: 
    return None  # Regenerar solo despuÃ©s de 7 dÃ­as
```

#### 3.2 Pre-generar Reportes
Crear un Cloud Scheduler que genere reportes del mes anterior cada 1ro del mes:

```python
# Script de pre-generaciÃ³n (ejecuta a las 2am del dÃ­a 1)
# Genera reporte del mes anterior y lo cachea
report = generate_executive_report(previous_month)
# Las llamadas futuras reutilizarÃ¡n el cache
```

**Beneficio:** Los reportes frecuentes (mensuales) ya estarÃ¡n cacheados

---

### SoluciÃ³n 4: **Aumentar Cuota en Vertex AI** (Operacional) ğŸ“ˆ

#### Pasos:
1. Ir a Google Cloud Console â†’ Vertex AI â†’ Quotas
2. Solicitar aumento de cuota para:
   - **Requests per minute (RPM):** De 60 â†’ 300  
   - **Tokens per minute (TPM):** De 2M â†’ 10M

**Beneficio:** Permite generar mÃ¡s narrativas simultÃ¡neamente

---

### SoluciÃ³n 5: **Fallback Graceful** (User Experience) ğŸ¨

En lugar de mostrar `"[AI Narrative Unavailable]"`, generar un placeholder contextual:

```python
# En lÃ­nea 121 de executive_insights.py
except google.api_core.exceptions.ResourceExhausted:
    logger.error("âŒ Quota exceeded after retries.")
    # En lugar de retornar texto genÃ©rico, usar datos del contexto
    return self._generate_fallback_narrative(section_name, data_context)

def _generate_fallback_narrative(self, section: str, data: Dict) -> str:
    """Genera narrativa bÃ¡sica sin LLM cuando hay errores de cuota."""
    if section == "critical_insight":
        actual_rate = data.get('headline_actual', {}).get('tasa', 0)
        prev_rate = data.get('headline_prev', {}).get('tasa', 0)
        delta = actual_rate - prev_rate
        
        trend = "incremento" if delta > 0 else "reducciÃ³n"
        return f"La tasa de rotaciÃ³n actual es {actual_rate:.1f}%, representando un {trend} de {abs(delta):.1f} puntos respecto al perÃ­odo anterior."
    
    # Similar logic for other sections...
    return "Narrativa AI temporalmente no disponible. Los datos visuales estÃ¡n disponibles arriba."
```

**Beneficio:** Mejor UX - el usuario ve al menos un resumen bÃ¡sico en lugar de un error

---

## ğŸš€ Plan de ImplementaciÃ³n Recomendado

###  Fase 1: Quick Wins (Hoy) âœ…
1. âœ… Aumentar retry attempts: 3 â†’ 5
2. âœ… Aumentar max wait time: 10s â†’ 60s
3. âœ… Agregar `time.sleep(2)` entre narrativas del orchestrator

### Fase 2: Mejoras Estructurales (Esta Semana) ğŸ”§
4. Activar TTL de cache (7 dÃ­as)
5. Implementar fallback narratives contextuales
6. Solicitar aumento de cuota en Vertex AI

### Fase 3: ArquitectÃ³nico (PrÃ³ximo Sprint) ğŸ—ï¸
7. Implementar Cloud Scheduler para pre-generaciÃ³n
8. Dashboard de monitoreo de cuota/uso de API

---

## ğŸ“ Archivos a Modificar

### 1. `app/ai/tools/executive_insights.py`
- LÃ­neas 80-84: Aumentar retry config
- LÃ­neas 119-124: Mejorar mensajes de error
- Agregar: `_generate_fallback_narrative()`

### 2. `app/ai/tools/executive_report_orchestrator.py`
- DespuÃ©s de cada `ai_gen.generate_section_insight()`: Agregar `time.sleep(2)`

---

## âœ… Criterios de AceptaciÃ³n

DespuÃ©s de implementar Fase 1:

- [ ] Reporte ejecutivo de aÃ±o completo (2025) genera **mÃ­nimo 3/6 narrativas** exitosas
- [ ] Errores 429 se retrantan automÃ¡ticamente con espera exponencial  
- [ ] Logs muestran `"ğŸ”„ Retrying after 429..."` en lugar de fallar inmediatamente
- [ ] Cache hits reducen llamadas nuevas al LLM en **>70%**

---

## ğŸ¯ PrÃ³ximos Pasos Inmediatos

1. **Implementar SoluciÃ³n 1** (aumentar retry wait)
2. **Implementar SoluciÃ³n 2** (rate limiting)
3. **Re-ejecutar test**: `python tests/test_narrative_diagnosis.py --period 2025`
4. **Validar mejora**: Al menos 50% de narrativas deben generarse exitosamente
